{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJECT 4:\n",
    "\n",
    "## Unsupervised Machine Learning - Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a part of my fourth project from the IBM Machine Learning certificate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main objective of this project is to cluster the data on this dataset and see which clustering algorithm is best for this purpose. The data from this dataset will be used to develop a customer segmentation to define marketing strategy. It summarizes the usage behavior of about 9000 active credit card holders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source Data: https://www.kaggle.com/arjunbhasin2013/ccdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns, os\n",
    "os.chdir('data')\n",
    "from colorsetup import colors, palette\n",
    "sns.set_palette(palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import and take a preliminary look at the dataset\n",
    "data = pd.read_csv('CC GENERAL.csv')\n",
    "\n",
    "data.head(4).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data.copy() # Keep a copy of our original data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of rows in the data:\", data.shape[0])\n",
    "print(\"Number of columns in the data:\", data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have determined the following:\n",
    "\n",
    "    - There are 18 columns and 8950 rows in this dataset. \n",
    "    - This dataset has 3 types of data: object, float64 and int64.  \n",
    "\n",
    "We will now do the following:\n",
    "\n",
    "    - Examine the correlation and skew of all of the variables -- except for the CUST_ID column, as it adds no value to our project\n",
    "    - Perform any appropriate feature transformations and/or scaling.\n",
    "    - Examine the pairwise distribution of the variables with pairplots to verify scaling and normalization efforts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We examine correlation between all variables excluding 'CUST_ID'\n",
    "num_columns = [x for x in data.columns if x not in ['CUST_ID']]\n",
    "\n",
    "# The correlation matrix\n",
    "corr_mat = data[num_columns].corr()\n",
    "\n",
    "# Strip out the diagonal values for the next step\n",
    "for x in range(len(num_columns)):\n",
    "    corr_mat.iloc[x,x] = 0.0\n",
    "    \n",
    "corr_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise maximal correlations. We see which feature values are mostly correlated with which ones.\n",
    "corr_mat.abs().idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see if there are any skew values in anticipation of transformations.\n",
    "skew_columns = (data[num_columns]\n",
    "                .skew()\n",
    "                .sort_values(ascending=False))\n",
    "\n",
    "skew_columns = skew_columns.loc[skew_columns > 0.75]\n",
    "skew_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We perform log transform on the skewed columns\n",
    "for col in skew_columns.index.tolist():\n",
    "    data[col] = np.log1p(data[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now perform feature scaling.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "data[num_columns] = sc.fit_transform(data[num_columns])\n",
    "\n",
    "data.head(4)\n",
    "# Now, all our values are scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now make a pairplot of the scaled and transformed data\n",
    "sns.pairplot(data, plot_kws=dict(alpha=.1, edgecolor='none'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('notebook')\n",
    "sns.pairplot(data[num_columns], \n",
    "             hue=None, \n",
    "             hue_order=['white', 'red'],\n",
    "             palette={'red', 'white', 'gray'});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pairplots show that the scaled and transformed values do not correlate with each other for the most part, except for features such as \"ONCEOFF_PURCHASES\" and \"PURCHASES\" for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We replace any NaN values in the dataset\n",
    "smaller_data = data.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Method 1: K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will fit a K-means clustering model with 2 clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(n_clusters=2, random_state=42)\n",
    "km = km.fit(smaller_data[num_columns])\n",
    "\n",
    "data['kmeans'] = km.predict(smaller_data[num_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data[['TENURE', 'kmeans']]\n",
    " .groupby(['kmeans', 'TENURE'])\n",
    " .size()\n",
    " .to_frame()\n",
    " .rename(columns={0:'number'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our dataset has been separated into 2 data clusters. The majority cluster is the \"0\" cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now fit K-Means models with cluster values ranging from 1 to 20 to determine which K-Means value we want to use.\n",
    "# Create and fit a range of models\n",
    "km_list = list()\n",
    "\n",
    "for clust in range(1,21):\n",
    "    km = KMeans(n_clusters=clust, random_state=42)\n",
    "    km = km.fit(smaller_data[num_columns])\n",
    "    \n",
    "    km_list.append(pd.Series({'clusters': clust, \n",
    "                              'inertia': km.inertia_,\n",
    "                              'model': km}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = (pd.concat(km_list, axis=1)\n",
    "             .T\n",
    "             [['clusters','inertia']]\n",
    "             .set_index('clusters'))\n",
    "\n",
    "ax = plot_data.plot(marker='o',ls='-')\n",
    "ax.set_xticks(range(0,21,2))\n",
    "ax.set_xlim(0,21)\n",
    "ax.set(xlabel='Cluster', ylabel='Inertia');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, I will choose 5 as my cluster value because it is the inflection point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see how our 5 clusters look\n",
    "km = KMeans(n_clusters=5, random_state=42)\n",
    "km = km.fit(smaller_data[num_columns])\n",
    "\n",
    "data['kmeans'] = km.predict(smaller_data[num_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data[['TENURE', 'kmeans']]\n",
    " .groupby(['kmeans', 'TENURE'])\n",
    " .size()\n",
    " .to_frame()\n",
    " .rename(columns={0:'number'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Method 2: Hierarchical Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now use the Agglomerative Clustering method on our dataset \n",
    "# We will then compare the results between this method and K-Means\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "ag = AgglomerativeClustering(n_clusters=2, linkage='ward', compute_full_tree=True) #We can set compute_full_tree value to False if we want to save computational time\n",
    "ag = ag.fit(smaller_data[num_columns])\n",
    "data['agglom'] = ag.fit_predict(smaller_data[num_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, for Agglomerative Clustering:\n",
    "(data[['TENURE','agglom','kmeans']]\n",
    " .groupby(['TENURE','agglom'])\n",
    " .size()\n",
    " .to_frame()\n",
    " .rename(columns={0:'number'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the clusters appear to be grouped accordingly. This method has managed to separate the results into two classes, \"0\" and \"1\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing with KMeans results:\n",
    "(data[['TENURE','agglom','kmeans']]\n",
    " .groupby(['TENURE','kmeans'])\n",
    " .size()\n",
    " .to_frame()\n",
    " .rename(columns={0:'number'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we compare the results:\n",
    "(data[['TENURE','agglom','kmeans']]\n",
    " .groupby(['TENURE','agglom','kmeans'])\n",
    " .size()\n",
    " .to_frame()\n",
    " .rename(columns={0:'number'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though the cluster numbers are not identical, the clusters themselves are very consistent. We will now plot a dendrogram created from the agglomerative clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy\n",
    "\n",
    "Z = hierarchy.linkage(ag.children_, method='ward')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "\n",
    "# Some color setup\n",
    "red = colors[2]\n",
    "blue = colors[0]\n",
    "\n",
    "hierarchy.set_link_color_palette([red, 'gray'])\n",
    "\n",
    "den = hierarchy.dendrogram(Z, orientation='top', \n",
    "                           p=50, truncate_mode='lastp',\n",
    "                           show_leaf_counts=True, ax=ax,\n",
    "                           above_threshold_color=blue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conclude that the Hierarchical Agglomerative Cluster model is the method best suited to this dataset. It has separated its results to into two classes and the dendogram shows us a clearer picture of how the clusters would be shaped."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "730f9183e7e55ca6b413db9e907f4351871a9884f15062ec558b8e495d8cf026"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
