{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJECT 3:\n",
    "\n",
    "## Supevised Machine Learning - Classification [African Country Recession (2000 to 2017)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a part of my third project from the IBM Machine Learning certificate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main objective of this analysis is to attempt to predict which factors contribute most to, or are most indicative of, recessions in Africa using classification. The target variable of this analysis is the ‘growthbucket’ variable and that column denotes either a \"1\" for \"Recession\" or \"0\" for \"No_Recession\". This dataset covers the period between the years 2000 and 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source Data: https://www.kaggle.com/chirin/african-country-recession-dataset-2000-to-2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We take a preliminary look at the data after loading it\n",
    "filepath = \"africa_recession.csv\"\n",
    "data = pd.read_csv(filepath)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of rows in the data:\", data.shape[0])\n",
    "print(\"Number of columns in the data:\", data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The different columns in the dataset\n",
    "print(data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The different types of data\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check to see if there are any missing values in each column\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get descriptive statistics of the dataset\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will begin by determining if our target variable is normally distributed using a histogram\n",
    "data.growthbucket.hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have determined the following:\n",
    "\n",
    "    - There are 50 columns and 486 rows in this dataset. \n",
    "    - This dataset has 2 types of data, float64 and int64. It has no object data type. \n",
    "    - There are no missing values in any of the columns.\n",
    "    - We also note that the target variable is heavily skewed towards \"0\" (No recession).\n",
    "    - We will use Log Transform to adjust the skewed data, this will decrease the effect of the outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a list of float colums to check for skewing\n",
    "mask = data.dtypes == float\n",
    "float_cols = data.columns[mask]\n",
    "\n",
    "skew_limit = 0.75 # define a limit above which we will log transform\n",
    "skew_vals = data[float_cols].skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then take a look at the skewed columns\n",
    "skew_cols = (skew_vals\n",
    "             .sort_values(ascending=False)\n",
    "             .to_frame()\n",
    "             .rename(columns={0:'Skew'})\n",
    "             .query('abs(Skew) > {}'.format(skew_limit)))\n",
    "\n",
    "skew_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at what happens to one of these features, when we apply np.log1p visually.\n",
    "\n",
    "# Choose a field\n",
    "field = \"pl_n\"\n",
    "\n",
    "# Create two \"subplots\" and a \"figure\" using matplotlib\n",
    "fig, (ax_before, ax_after) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Create a histogram on the \"ax_before\" subplot\n",
    "data[field].hist(ax=ax_before)\n",
    "\n",
    "# Apply a log transformation (numpy syntax) to this column\n",
    "data[field].apply(np.log1p).hist(ax=ax_after)\n",
    "\n",
    "# Formatting of titles etc. for each subplot\n",
    "ax_before.set(title='before np.log1p', ylabel='frequency', xlabel='value')\n",
    "ax_after.set(title='after np.log1p', ylabel='frequency', xlabel='value')\n",
    "fig.suptitle('Field \"{}\"'.format(field));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that the skew transformation makes a difference to features that need it\n",
    "# We then perform the skew transformation:\n",
    "\n",
    "for col in skew_cols.index.values:\n",
    "    if col == \"growthbucket\":\n",
    "        continue\n",
    "    data[col] = data[col].apply(np.log1p) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now apply basic feature transformations to some of the columns.\n",
    "smaller_data = data.loc[:,['pl_n', 'xr', 'pop', 'ck', 'rnna', 'csh_x', 'rkna', 'pl_g', 'delta',\n",
    "                      'ccon', 'cda', 'rconna', 'emp', 'cn', 'rdana', 'irr', 'pl_c', 'metals_minerals_change',\n",
    "                      'rtfpna', 'forestry_change', 'csh_i', 'pl_con', 'rwtfpna',\n",
    "                      'fish','csh_m', 'growthbucket']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we take a look at the summary statistics of the subset data\n",
    "smaller_data.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There appears to be no NaN values in the data set. Our dataset is perfectly filtered.\n",
    "# We will now generate pairplot visuals to better understand the target and feature-target relationships\n",
    "sns.pairplot(smaller_data, plot_kws=dict(alpha=.1, edgecolor='none'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the pairplot above we can see that the target variable does not seem to have a linear relationship with any of the features.\n",
    "We will now:\n",
    "\n",
    "    * Calculate the correlations between the dependent variables.\n",
    "    * Create a histogram of the correlation values\n",
    "    * Identify those that are most correlated (either positively or negatively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation values\n",
    "feature_cols = data.columns[:-1]\n",
    "corr_values = data[feature_cols].corr()\n",
    "corr_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplify by emptying all the data below the diagonal\n",
    "tril_index = np.tril_indices_from(corr_values)\n",
    "tril_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the unused values NaNs\n",
    "corr_array = np.array(corr_values)\n",
    "corr_array[np.tril_indices_from(corr_values)] = np.nan\n",
    "pd.DataFrame(corr_array)\n",
    "# now we have nulled out all the values on the diagonal and below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we recreate correlation pandas dataframe\n",
    "corr_values = pd.DataFrame(corr_array,columns = corr_values.columns, index= corr_values.index)\n",
    "corr_values.stack().to_frame().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We stack the data and convert to a data frame\n",
    "corr_values = (corr_values\n",
    "               .stack()\n",
    "               .to_frame()\n",
    "               .reset_index()\n",
    "               .rename(columns={'level_0':'feature1',\n",
    "                                'level_1':'feature2',\n",
    "                                0:'correlation'}))\n",
    "\n",
    "# Get the absolute values for sorting\n",
    "corr_values['abs_correlation'] = corr_values.correlation.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We show a histogram of the absolute value correlations.\n",
    "sns.set_context('talk')\n",
    "sns.set_style('white')\n",
    "\n",
    "ax = corr_values.abs_correlation.hist(bins=50, figsize=(12, 8))\n",
    "ax.set(xlabel='Absolute Correlation', ylabel='Frequency');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- These are the most highly correlated values.\n",
    "- Values are sorted by correlation going from top to bottom. We only want values from 0.8 onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_values.sort_values('correlation', ascending=False).query('abs_correlation>0.8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now split the data into train and test data sets before we try the Logistic Regression model.\n",
    "We will be using Scikit-learn's `StratifiedShuffleSplit` to maintain the same ratio of predictor classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Get the split indexes\n",
    "strat_shuf_split = StratifiedShuffleSplit(n_splits=1, \n",
    "                                          test_size=0.3, \n",
    "                                          random_state=42)\n",
    "\n",
    "train_idx, test_idx = next(strat_shuf_split.split(data[feature_cols], data.growthbucket))\n",
    "\n",
    "# Create the dataframes\n",
    "X_train = data.loc[train_idx, feature_cols]\n",
    "y_train = data.loc[train_idx, 'growthbucket']\n",
    "\n",
    "X_test  = data.loc[test_idx, feature_cols]\n",
    "y_test  = data.loc[test_idx, 'growthbucket']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Model 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We fit a logistic regression model without any regularization using all of the features\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Standard logistic regression\n",
    "lr = LogisticRegression(solver='liblinear').fit(X_train, y_train)\n",
    "# liblinear is the \"One vs the Rest\" method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "# L1 regularized logistic regression. We used LogisticRegressionCV because it works like cv grid search.\n",
    "# It will check against 10 default values of that CValue and then we can optimize from there.\n",
    "lr_l1 = LogisticRegressionCV(Cs=10, cv=5, penalty='l1', max_iter=1000, solver='liblinear').fit(X_train, y_train)\n",
    "\n",
    "# L2 regularized logistic regression\n",
    "lr_l2 = LogisticRegressionCV(Cs=10, cv=5, penalty='l2', max_iter=1000, solver='liblinear').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the coefficients into a dataframe\n",
    "coefficients = list()\n",
    "\n",
    "coeff_labels = ['lr', 'l1', 'l2']\n",
    "coeff_models = [lr, lr_l1, lr_l2]\n",
    "\n",
    "for lab,mod in zip(coeff_labels, coeff_models):\n",
    "    coeffs = mod.coef_\n",
    "    coeff_label = pd.MultiIndex(levels=[[lab], [0]], \n",
    "                                 codes=[[0], [0]])\n",
    "    coefficients.append(pd.DataFrame(coeffs.T, columns=coeff_label))\n",
    "\n",
    "coefficients = pd.concat(coefficients, axis=1)\n",
    "\n",
    "coefficients.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the class and the probability for each model\n",
    "y_pred = list()\n",
    "y_prob = list()\n",
    "\n",
    "coeff_labels = ['lr', 'l1', 'l2']\n",
    "coeff_models = [lr, lr_l1, lr_l2]\n",
    "\n",
    "for lab,mod in zip(coeff_labels, coeff_models):\n",
    "    y_pred.append(pd.Series(mod.predict(X_test), name=lab))\n",
    "    y_prob.append(pd.Series(mod.predict_proba(X_test).max(axis=1), name=lab))\n",
    "    \n",
    "y_pred = pd.concat(y_pred, axis=1)\n",
    "y_prob = pd.concat(y_prob, axis=1)\n",
    "\n",
    "y_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each model, we calculate the following error metrics: \n",
    "\n",
    "* Accuracy\n",
    "* Precision\n",
    "* Recall\n",
    "* F-score\n",
    "* Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "metrics = list()\n",
    "cm = dict()\n",
    "\n",
    "for lab in coeff_labels:\n",
    "\n",
    "    # Precision, recall, f-score from the multi-class support function\n",
    "    precision, recall, fscore, _ = score(y_test, y_pred[lab], average='micro')\n",
    "    \n",
    "    # The usual way to calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred[lab])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Last, the confusion matrix\n",
    "    cm[lab] = confusion_matrix(y_test, y_pred[lab])\n",
    "    \n",
    "    metrics.append(pd.Series({'precision':precision, 'recall':recall, \n",
    "                              'fscore':fscore, 'accuracy':accuracy\n",
    "                             }, \n",
    "                             name=lab))\n",
    "\n",
    "metrics = pd.concat(metrics, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the precision, recall, f score and accuracy are the same due to how we configured the Logistic Regression algorithm. \n",
    "The percentage rate for this model is 92.46% though, which is relatively high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Model 2: K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe().T\n",
    "# We notice that the variables are on different scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We scale the data \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mm = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(data.describe().T, 3)\n",
    "# Now they are all on a similar scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, f1_score\n",
    "\n",
    "# Estimate KNN model and report outcomes\n",
    "knn = KNeighborsClassifier(n_neighbors=4)\n",
    "knn = knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "# Precision, recall, f-score from the multi-class support function\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('Accuracy score: ', round(accuracy_score(y_test, y_pred), 2))\n",
    "print('F1 Score: ', round(f1_score(y_test, y_pred), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "# The blue squares are the incorrect values that we predicted\n",
    "sns.set_palette(sns.color_palette(colors))\n",
    "_, ax = plt.subplots(figsize=(12,12))\n",
    "ax = sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap=colors, annot_kws={\"size\": 40, \"weight\": \"bold\"})  \n",
    "labels = ['False', 'True']\n",
    "ax.set_xticklabels(labels, fontsize=25);\n",
    "ax.set_yticklabels(labels[::-1], fontsize=25);\n",
    "ax.set_ylabel('Prediction', fontsize=30);\n",
    "ax.set_xlabel('Ground Truth', fontsize=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noted that the accuracy score for this KNN model is 92.00%, which is lower than the Logistic Regression model's score. The F1 score is also low (15%).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Model 3: Random Forest and Out Of Bag Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will fit random forest models with a range of tree numbers and evaluate the out-of-bag error for each of these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings about too few trees from the early models\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the random forest estimator\n",
    "\n",
    "RF = RandomForestClassifier(oob_score=True, \n",
    "                            random_state=42, \n",
    "                            warm_start=True,\n",
    "                            n_jobs=-1)\n",
    "\n",
    "oob_list = list()\n",
    "\n",
    "# Iterate through all of the possibilities for \n",
    "# number of trees\n",
    "for n_trees in [15, 20, 30, 40, 50, 100, 150, 200, 300, 400]:\n",
    "    \n",
    "    # Use this to set the number of trees\n",
    "    RF.set_params(n_estimators=n_trees)\n",
    "\n",
    "    # Fit the model\n",
    "    RF.fit(X_train, y_train)\n",
    "\n",
    "    # Get the oob error\n",
    "    oob_error = 1 - RF.oob_score_\n",
    "    \n",
    "    # Store it\n",
    "    oob_list.append(pd.Series({'n_trees': n_trees, 'oob': oob_error}))\n",
    "\n",
    "rf_oob_df = pd.concat(oob_list, axis=1).T.set_index('n_trees')\n",
    "\n",
    "rf_oob_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "sns.set_style('white')\n",
    "\n",
    "ax = rf_oob_df.plot(legend=False, marker='o', figsize=(14, 7), linewidth=5)\n",
    "ax.set(ylabel='out-of-bag error');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error looks like it has stabilized at around 40 trees. This is where it plateaus. We will now use extra randomized trees (`ExtraTreesClassifier`). We will then compare the out-of-bag errors for the two different types of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# Initialize the random forest estimator\n",
    "\n",
    "EF = ExtraTreesClassifier(oob_score=True, \n",
    "                          random_state=42, \n",
    "                          warm_start=True,\n",
    "                          bootstrap=True,\n",
    "                          n_jobs=-1)\n",
    "\n",
    "oob_list = list()\n",
    "\n",
    "# Iterate through all of the possibilities for \n",
    "# number of trees\n",
    "for n_trees in [15, 20, 30, 40, 50, 100, 150, 200, 300, 400]:\n",
    "    \n",
    "    # Use this to set the number of trees\n",
    "    EF.set_params(n_estimators=n_trees)\n",
    "    EF.fit(X_train, y_train)\n",
    "\n",
    "    # oob error\n",
    "    oob_error = 1 - EF.oob_score_\n",
    "    oob_list.append(pd.Series({'n_trees': n_trees, 'oob': oob_error}))\n",
    "\n",
    "et_oob_df = pd.concat(oob_list, axis=1).T.set_index('n_trees')\n",
    "\n",
    "et_oob_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will combine the two dataframes into a single one for easier plotting.\n",
    "oob_df = pd.concat([rf_oob_df.rename(columns={'oob':'RandomForest'}),\n",
    "                    et_oob_df.rename(columns={'oob':'ExtraTrees'})], axis=1)\n",
    "\n",
    "oob_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "sns.set_style('white')\n",
    "\n",
    "ax = oob_df.plot(marker='o', figsize=(14, 7), linewidth=5)\n",
    "ax.set(ylabel='out-of-bag error');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the extra randomized trees model performs better than the random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score\n",
    "\n",
    "cr = classification_report(y_test, y_pred)\n",
    "print(cr)\n",
    "\n",
    "score_df = pd.DataFrame({'accuracy': accuracy_score(y_test, y_pred),\n",
    "                         'precision': precision_score(y_test, y_pred),\n",
    "                         'recall': recall_score(y_test, y_pred),\n",
    "                         'f1': f1_score(y_test, y_pred),\n",
    "                         'auc': roc_auc_score(y_test, y_pred)},\n",
    "                         index=pd.Index([0]))\n",
    "\n",
    "print(score_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the precision, f1, support score and recall for \"0\" are high but low for \"1\". The general accuracy score is 91.77% which is lower than the Logistic Regression and the KNN models' scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "_, ax = plt.subplots(figsize=(12,12))\n",
    "ax = sns.heatmap(cm, annot=True, fmt='d', cmap=colors, annot_kws={\"size\": 40, \"weight\": \"bold\"})\n",
    "\n",
    "labels = ['False', 'True']\n",
    "ax.set_xticklabels(labels, fontsize=25);\n",
    "ax.set_yticklabels(labels[::-1], fontsize=25);\n",
    "ax.set_ylabel('Prediction', fontsize=30);\n",
    "ax.set_xlabel('Ground Truth', fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, precision_recall_curve, confusion_matrix\n",
    "sns.set_context('talk')\n",
    "\n",
    "fig, axList = plt.subplots(ncols=2)\n",
    "fig.set_size_inches(16, 8)\n",
    "\n",
    "# Get the probabilities for each of the two categories\n",
    "y_prob = model.predict_proba(X_test)\n",
    "\n",
    "# Plot the ROC-AUC curve\n",
    "ax = axList[0]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob[:,1])\n",
    "ax.plot(fpr, tpr, color=colors[0], linewidth=5)\n",
    "# It is customary to draw a diagonal dotted line in ROC plots.\n",
    "# This is to indicate completely random prediction. Deviation from this\n",
    "# dotted line towards the upper left corner signifies the power of the model.\n",
    "ax.plot([0, 1], [0, 1], ls='--', color='black', lw=.3)\n",
    "ax.set(xlabel='False Positive Rate',\n",
    "       ylabel='True Positive Rate',\n",
    "       xlim=[-.01, 1.01], ylim=[-.01, 1.01],\n",
    "       title='ROC curve')\n",
    "ax.grid(True)\n",
    "\n",
    "# Plot the precision-recall curve\n",
    "ax = axList[1]\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_prob[:,1])\n",
    "ax.plot(recall, precision, color=colors[1], linewidth=5)\n",
    "ax.set(xlabel='Recall', ylabel='Precision',\n",
    "       xlim=[-.01, 1.01], ylim=[-.01, 1.01],\n",
    "       title='Precision-Recall curve')\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the ROC curve is far from the top left corner so the test is not efficient. This makes sense since this data is imbalanced. The Precision recall curve is also not efficient for this data. This is all due to the data's accuracy being low for the \"1\" class as noted before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = pd.Series(model.feature_importances_, index=feature_cols).sort_values(ascending=False)\n",
    "\n",
    "ax = feature_imp.plot(kind='bar', figsize=(16, 6))\n",
    "ax.set(ylabel='Relative Importance');\n",
    "ax.set(ylabel='Feature');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above bar chart shows us which features are the most important when it comes to determining which features have the most influence on the target variable. In this case it is the \"rwtfpna\" (TFP at constant national prices (2011=1) feature. This feature is basically the portion of output that is not explained by the amount of inputs used in production for the reference year 2011."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also conclude that the best model to use on this dataset is the Logistic Regression model. It has the highest accuracy score, although not by much as compared to the KNN and Random Trees models. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "730f9183e7e55ca6b413db9e907f4351871a9884f15062ec558b8e495d8cf026"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
